{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Enhancement with Convolutional Neural Network.","metadata":{"_uuid":"d524913cbf7d3ad074e641df5c5889453fda18ca"}},{"cell_type":"markdown","source":"## **What is the history of CNN?**\nConvolutional neural networks, also called ConvNets, were first introduced in the 1980s by Yann LeCun, a postdoctoral computer science researcher. The early version of CNNs, called LeNet (after LeCun), could recognize handwritten digits. The availability of large sets of data, namely the ImageNet dataset with millions of labeled pictures, and vast compute resources enabled researchers to create complex CNNs that could perform computer vision tasks that were previously impossible. ","metadata":{"_uuid":"0ccc48e351037510326416ea4ef1310d9d979107"}},{"cell_type":"markdown","source":"## **What is CNN?**\nA Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics. The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. This is important when we are to design an architecture which is not only good at learning features but also is scalable to massive datasets.\n","metadata":{}},{"cell_type":"markdown","source":"## **How do CNN work**\nConvolutional neural networks are composed of multiple layers of artificial neurons. Artificial neurons, a rough imitation of their biological counterparts, are mathematical functions that calculate the weighted sum of multiple inputs and outputs an activation value. The first (or bottom) layer of the CNN usually detects basic features such as horizontal, vertical, and diagonal edges. The output of the first layer is fed as input of the next layer, which extracts more complex features, such as corners and combinations of edges. As you move deeper into the convolutional neural network, the layers start detecting higher-level features such as objects, faces, and more. The behavior of each neuron is defined by its weights. When fed with the pixel values, the artificial neurons of a CNN pick out various visual features.\nWhen you input an image into a ConvNet, each of its layers generates several activation maps. Activation maps highlight the relevant features of the image. Each of the neurons takes a patch of pixels as input, multiplies their color values by its weights, sums them up, and runs them through the activation function.\nAs you move deeper into the convolutional neural network, the layers start detecting higher-level features such as objects, faces, and more.  \n\n![](https://editor.analyticsvidhya.com/uploads/25366Convolutional_Neural_Network_to_identify_the_image_of_a_bird.png)","metadata":{}},{"cell_type":"markdown","source":"## Background reading\n* **API** : An application programming interface (API) is a connection between computers or between computer programs. It is a type of software interface, offering a service to other pieces of software.\n* **Keras** : Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. It also has extensive documentation and developer guides.\n* **Inception-ResNet-v2** : It is a convolutional neural network that is trained on more than a million images from the ImageNet database [1]. The network is 164 layers deep and can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. \n![](https://929687.smushcdn.com/2633864/wp-content/uploads/2018/12/keras_conv2d_inception_module.png?lossy=1&strip=1&webp=1)","metadata":{}},{"cell_type":"markdown","source":"### **The Inception-ResNet-v2**:\nArchitecture is more accurate than previous state of the art models, as shown in the table below, which reports the Top-1 and Top-5 validation accuracies on the ILSVRC 2012 image classification benchmark based on a single crop of the image. Furthermore, this new model only requires roughly twice the memory and computation compared to Inception V3.\\\n\\\n![](https://1.bp.blogspot.com/-O7AznVGY9js/V8cV_wKKsMI/AAAAAAAABKQ/maO7n2w3dT4Pkcmk7wgGqiSX5FUW2sfZgCLcB/s640/image00.png)","metadata":{}},{"cell_type":"markdown","source":"[](https://miro.medium.com/max/1114/1*RzvmmEQH_87qKWYBFIG_DA.png)","metadata":{}},{"cell_type":"markdown","source":"***The aim of this project is to enhance low light image with salt & pepper noise using CNN architecture InceptionResNetV2 which is already a pretrained keras model.***","metadata":{}},{"cell_type":"markdown","source":"## About the input:\nThe inputs provided to the model is:\n* image-classification dataset: It contains 3 directories; images, test and validation, with each having 4 further directories - art and culture, food, travel and adventure, architecture.\n* Keras-pretrained models : Contains pretrained InceptionResNetV2 model used for the enhancement.\n* real-low-light: A dataset containing real life low light images that I clicked on my phone.","metadata":{}},{"cell_type":"markdown","source":"## **Importing necessary libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import preprocess_input,decode_predictions\nfrom keras import backend as K\nfrom keras.layers import add, Conv2D,MaxPooling2D,UpSampling2D,Input,BatchNormalization, RepeatVector, Reshape\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow import set_random_seed\nset_random_seed(2)\nnp.random.seed(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-12T10:12:23.976307Z","iopub.execute_input":"2022-08-12T10:12:23.976677Z","iopub.status.idle":"2022-08-12T10:12:27.082953Z","shell.execute_reply.started":"2022-08-12T10:12:23.976615Z","shell.execute_reply":"2022-08-12T10:12:27.082228Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Now we will print the list of all files and directories in the specified input directory. The dataset used can be accesed by clicking [here](https://www.kaggle.com/datasets/duttadebadri/image-classification?select=validation)**","metadata":{}},{"cell_type":"code","source":"print(os.listdir(\"../input\"))","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:12:27.084411Z","iopub.execute_input":"2022-08-12T10:12:27.084715Z","iopub.status.idle":"2022-08-12T10:12:27.091148Z","shell.execute_reply.started":"2022-08-12T10:12:27.084670Z","shell.execute_reply":"2022-08-12T10:12:27.090338Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Here we are providing the art and culture directory as input .","metadata":{}},{"cell_type":"code","source":"InputPath=\"../input/image-classification/images/images/travel and  adventure/\"","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-08-12T10:12:27.092478Z","iopub.execute_input":"2022-08-12T10:12:27.092996Z","iopub.status.idle":"2022-08-12T10:12:27.100403Z","shell.execute_reply.started":"2022-08-12T10:12:27.092879Z","shell.execute_reply":"2022-08-12T10:12:27.099690Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The following snippet of code is to add noise into the image in a salt & pepper format.","metadata":{}},{"cell_type":"code","source":"def noisy(noise_typ,image):\n    if noise_typ == \"gauss\":\n        row,col,ch= image.shape\n        mean = 0\n        var = 0.0001\n        sigma = var**0.05\n        gauss = np.random.normal(mean,sigma,(row,col,ch))\n        gauss = gauss.reshape(row,col,ch)\n        noisy =  gauss + image\n        return noisy\n    elif noise_typ == \"s&p\":\n        row,col,ch = image.shape\n        s_vs_p = 0.5\n        amount = 1.0\n        out = np.copy(image)\n        # Salt mode\n        num_salt = np.ceil(image.size * s_vs_p)\n        coords = [np.random.randint(0, i, int(num_salt))\n              for i in image.shape]\n        out[coords] = 1\n\n        # Pepper mode\n        num_pepper = np.ceil(image.size * (1. - s_vs_p))\n        coords = [np.random.randint(0, i , int(num_pepper))\n              for i in image.shape]\n        out[coords] = 1\n        return out\n    elif noise_typ == \"poisson\":\n        vals = len(np.unique(image))\n        vals = 2 ** np.ceil(np.log2(vals))\n        noisy = np.random.poisson(image * vals) / float(vals)\n        return noisy\n    elif noise_typ ==\"speckle\":\n        row,col,ch = image.shape\n        gauss = np.random.randn(row,col,ch)\n        gauss = gauss.reshape(row,col,ch)        \n        noisy = image + image * gauss\n        return noisy","metadata":{"_uuid":"fdded9f215e010774ef3226154e8b879d27f075a","execution":{"iopub.status.busy":"2022-08-12T10:12:27.101895Z","iopub.execute_input":"2022-08-12T10:12:27.102310Z","iopub.status.idle":"2022-08-12T10:12:27.113805Z","shell.execute_reply.started":"2022-08-12T10:12:27.102169Z","shell.execute_reply":"2022-08-12T10:12:27.113034Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## True Image","metadata":{"_uuid":"3f7716823b7942e4ebc6bd2e3681551bfd739482"}},{"cell_type":"code","source":"img = cv.imread(InputPath+\"Places365_val_00005734.jpg\")  \nimg = cv.cvtColor(img, cv.COLOR_BGR2RGB)\nl = img.max()\nplt.imshow(img)","metadata":{"_uuid":"2aff9f430e19321ffedd3b8ec2699ceff0ed3573","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-08-12T10:12:27.115421Z","iopub.execute_input":"2022-08-12T10:12:27.115884Z","iopub.status.idle":"2022-08-12T10:12:27.449673Z","shell.execute_reply.started":"2022-08-12T10:12:27.115649Z","shell.execute_reply":"2022-08-12T10:12:27.448802Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Image after adding the salt & pepper noise.","metadata":{"_uuid":"55bf64c4ba28c041f45c506c123be3b9aa0a745c"}},{"cell_type":"code","source":"Noise = noisy(\"s&p\",img)\nplt.imshow(Noise)","metadata":{"_uuid":"f8081a244f52681bc2a428f1b55ed3d044b07d54","execution":{"iopub.status.busy":"2022-08-12T10:12:27.450673Z","iopub.execute_input":"2022-08-12T10:12:27.450954Z","iopub.status.idle":"2022-08-12T10:12:27.771003Z","shell.execute_reply.started":"2022-08-12T10:12:27.450895Z","shell.execute_reply":"2022-08-12T10:12:27.769883Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## **Converting the image to low light image with salt & pepper noise.**","metadata":{"_uuid":"f6f0b2304ee00743a78752fbc0ac4d26637ddaa2"}},{"cell_type":"markdown","source":"***cvtColor() is a color conversion code, The enumerator COLOR_BGR2HSV converts RGB/BGR to HSV (hue saturation value).***","metadata":{}},{"cell_type":"code","source":"hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV) #convert it to hsv\nhsv[...,2] = hsv[...,2]*0.2\nimg1 = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\nNoise2 = noisy(\"s&p\",img1)\n\nplt.imshow(Noise2)","metadata":{"_uuid":"7ce64be9f0164f864965e12c8be09957b316794b","execution":{"iopub.status.busy":"2022-08-12T10:12:27.772455Z","iopub.execute_input":"2022-08-12T10:12:27.772867Z","iopub.status.idle":"2022-08-12T10:12:28.106777Z","shell.execute_reply.started":"2022-08-12T10:12:27.772805Z","shell.execute_reply":"2022-08-12T10:12:28.106028Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We convert all images in input directory to low light ans salt & pepper noise images.","metadata":{}},{"cell_type":"code","source":"def PreProcessData(ImagePath):\n    X_=[]\n    y_=[]\n    count=0\n    for imageDir in os.listdir(ImagePath):\n        if count<2131:\n            try:\n                count=count+1\n                img = cv.imread(ImagePath + imageDir)\n                img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n                img_y = cv.resize(img,(500,500))\n                hsv = cv.cvtColor(img_y, cv.COLOR_BGR2HSV) #convert it to hsv\n                hsv[...,2] = hsv[...,2]*0.2\n                img_1 = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n                Noisey_img = noisy(\"s&p\",img_1)\n                X_.append(Noisey_img)\n                y_.append(img_y)\n            except:\n                pass\n    X_ = np.array(X_)\n    y_ = np.array(y_)\n    \n    return X_,y_","metadata":{"_uuid":"6a900f93fbacd86607c71e7f4917eae0a2a5d8b8","execution":{"iopub.status.busy":"2022-08-12T10:12:28.107976Z","iopub.execute_input":"2022-08-12T10:12:28.108415Z","iopub.status.idle":"2022-08-12T10:12:28.116613Z","shell.execute_reply.started":"2022-08-12T10:12:28.108355Z","shell.execute_reply":"2022-08-12T10:12:28.115330Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## PreProcessing all elements under InputPath","metadata":{}},{"cell_type":"code","source":"X_,y_ = PreProcessData(InputPath)","metadata":{"_uuid":"1b1d4fd22d066c57f2b42d7a936a6d8970e46a24","execution":{"iopub.status.busy":"2022-08-12T10:12:28.118328Z","iopub.execute_input":"2022-08-12T10:12:28.119016Z","iopub.status.idle":"2022-08-12T10:14:24.672509Z","shell.execute_reply.started":"2022-08-12T10:12:28.118947Z","shell.execute_reply":"2022-08-12T10:14:24.671534Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":" ### **Keras.Conv2D Class**\nKeras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs. The Conv2D class has the following arguments\n> Conv2D(filters, kernel_size, padding='valid', activation=None, strides)\n\n\n![](https://929687.smushcdn.com/2633864/wp-content/uploads/2018/12/keras_conv2d_num_filters.png?lossy=1&strip=1&webp=1)","metadata":{}},{"cell_type":"markdown","source":"[](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2FConvolutional-Neural-Network-model-visualization_fig3_346475943&psig=AOvVaw2mQFXucpWmqUTgtAJCFx3h&ust=1648401902487000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCJCWv5qm5PYCFQAAAAAdAAAAABAK)","metadata":{}},{"cell_type":"code","source":"K.clear_session()\ndef InstantiateModel(in_):\n    \n    model_1 = Conv2D(16,(3,3), activation='relu',padding='same',strides=1)(in_)\n    model_1 = Conv2D(32,(3,3), activation='relu',padding='same',strides=1)(model_1)\n    model_1 = Conv2D(64,(2,2), activation='relu',padding='same',strides=1)(model_1)\n    \n    model_2 = Conv2D(32,(3,3), activation='relu',padding='same',strides=1)(in_)\n    model_2 = Conv2D(64,(2,2), activation='relu',padding='same',strides=1)(model_2)\n    \n    model_2_0 = Conv2D(64,(2,2), activation='relu',padding='same',strides=1)(model_2)\n    \n    model_add = add([model_1,model_2,model_2_0])\n    \n    model_3 = Conv2D(64,(3,3), activation='relu',padding='same',strides=1)(model_add)\n    model_3 = Conv2D(32,(3,3), activation='relu',padding='same',strides=1)(model_3)\n    model_3 = Conv2D(16,(2,2), activation='relu',padding='same',strides=1)(model_3)\n    \n    model_3_1 = Conv2D(32,(3,3), activation='relu',padding='same',strides=1)(model_add)\n    model_3_1 = Conv2D(16,(2,2), activation='relu',padding='same',strides=1)(model_3_1)\n    \n    model_3_2 = Conv2D(16,(2,2), activation='relu',padding='same',strides=1)(model_add)\n    \n    model_add_2 = add([model_3_1,model_3_2,model_3])\n    \n    model_4 = Conv2D(16,(3,3), activation='relu',padding='same',strides=1)(model_add_2)\n    model_4_1 = Conv2D(16,(3,3), activation='relu',padding='same',strides=1)(model_add)\n    #Extension\n    model_add_3 = add([model_4_1,model_add_2,model_4])\n    \n    model_5 = Conv2D(16,(3,3), activation='relu',padding='same',strides=1)(model_add_3)\n    model_5 = Conv2D(16,(2,2), activation='relu',padding='same',strides=1)(model_add_3)\n    \n    model_5 = Conv2D(3,(3,3), activation='relu',padding='same',strides=1)(model_5)\n    \n    return model_5","metadata":{"_uuid":"dffd0b9c2f6eca271445b81cfc65204b7f5ea1bd","execution":{"iopub.status.busy":"2022-08-12T10:14:24.673936Z","iopub.execute_input":"2022-08-12T10:14:24.674218Z","iopub.status.idle":"2022-08-12T10:14:24.690846Z","shell.execute_reply.started":"2022-08-12T10:14:24.674171Z","shell.execute_reply":"2022-08-12T10:14:24.690241Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now we will pass our input into the function we just created.","metadata":{}},{"cell_type":"code","source":"Input_Sample = Input(shape=(500, 500,3))\nOutput_ = InstantiateModel(Input_Sample)\nModel_Enhancer = Model(inputs=Input_Sample, outputs=Output_)","metadata":{"_uuid":"58d771d6fadc13c066bee1ffcb582b65fb1e5c2f","execution":{"iopub.status.busy":"2022-08-12T10:14:24.692478Z","iopub.execute_input":"2022-08-12T10:14:24.693077Z","iopub.status.idle":"2022-08-12T10:14:25.031115Z","shell.execute_reply.started":"2022-08-12T10:14:24.692777Z","shell.execute_reply":"2022-08-12T10:14:25.029498Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Now we will use an optimizer to compile the keras model. Here I am going with the adam optimizer, as it provided the least loss among all other optimizer while running.","metadata":{}},{"cell_type":"code","source":"Model_Enhancer.compile(optimizer=\"adam\", loss='mean_squared_error', metrics = ['acc'])\nModel_Enhancer.summary()","metadata":{"_uuid":"2f15f4efbaf048cce7e520102bb0e3593b4a8239","execution":{"iopub.status.busy":"2022-08-12T10:14:25.032974Z","iopub.execute_input":"2022-08-12T10:14:25.033474Z","iopub.status.idle":"2022-08-12T10:14:25.115788Z","shell.execute_reply.started":"2022-08-12T10:14:25.033421Z","shell.execute_reply":"2022-08-12T10:14:25.114790Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"A visual representation of the model is shown below.","metadata":{}},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(Model_Enhancer,to_file='model_.png',show_shapes=True, show_layer_names=True)\nfrom IPython.display import Image\nImage(retina=True, filename='model_.png')","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:14:25.117046Z","iopub.execute_input":"2022-08-12T10:14:25.117362Z","iopub.status.idle":"2022-08-12T10:14:26.722123Z","shell.execute_reply.started":"2022-08-12T10:14:25.117314Z","shell.execute_reply":"2022-08-12T10:14:26.721205Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(X_)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:14:26.723833Z","iopub.execute_input":"2022-08-12T10:14:26.724359Z","iopub.status.idle":"2022-08-12T10:14:26.732154Z","shell.execute_reply.started":"2022-08-12T10:14:26.724302Z","shell.execute_reply":"2022-08-12T10:14:26.730905Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"****keras.fit_generator() method: The model is trained on batch-by-batch data generated by the Python constructor. Here I have given epoch value and steps_per_epoch value as 53 and 39 to get a more accurate graph. These values were chosen via trial and error****","metadata":{}},{"cell_type":"code","source":"def GenerateInputs(X,y):\n    for i in range(len(X)):\n        X_input = X[i].reshape(1,500,500,3)\n        y_input = y[i].reshape(1,500,500,3)\n        yield (X_input,y_input)\ntrainmodel = Model_Enhancer.fit_generator(GenerateInputs(X_,y_),epochs=53,verbose=2,steps_per_epoch=39,shuffle=True, use_multiprocessing = False)","metadata":{"_uuid":"3c8e442f79e1c5cadb015ba2a26c329ab8c86356","execution":{"iopub.status.busy":"2022-08-12T10:14:26.733504Z","iopub.execute_input":"2022-08-12T10:14:26.734013Z","iopub.status.idle":"2022-08-12T10:16:43.197509Z","shell.execute_reply.started":"2022-08-12T10:14:26.733939Z","shell.execute_reply":"2022-08-12T10:16:43.196559Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that as the number of epochs increase, the accuracy also increases.","metadata":{}},{"cell_type":"code","source":"plt.plot(trainmodel.history['acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:43.199091Z","iopub.execute_input":"2022-08-12T10:16:43.199413Z","iopub.status.idle":"2022-08-12T10:16:43.408877Z","shell.execute_reply.started":"2022-08-12T10:16:43.199362Z","shell.execute_reply":"2022-08-12T10:16:43.407925Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Here we see that as the number of epochs increases, the loss decreases.","metadata":{}},{"cell_type":"code","source":"plt.plot(trainmodel.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:43.410175Z","iopub.execute_input":"2022-08-12T10:16:43.410695Z","iopub.status.idle":"2022-08-12T10:16:43.648590Z","shell.execute_reply.started":"2022-08-12T10:16:43.410642Z","shell.execute_reply":"2022-08-12T10:16:43.647671Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**As expected we can see that the value of Loss decreases as number of epochs increases, while the acuuracy increases with the number of epochs**","metadata":{}},{"cell_type":"code","source":"TestPath=\"../input/image-classification/images/images/art and culture/\"","metadata":{"_uuid":"49c395c3436b12a70cc00a68d2f77ba75ebf5220","execution":{"iopub.status.busy":"2022-08-12T10:16:43.650024Z","iopub.execute_input":"2022-08-12T10:16:43.650549Z","iopub.status.idle":"2022-08-12T10:16:43.654645Z","shell.execute_reply.started":"2022-08-12T10:16:43.650499Z","shell.execute_reply":"2022-08-12T10:16:43.653817Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The below function returns a low light noise filled image from the test input and run it thorugh the model enhanceer we created earlier.","metadata":{}},{"cell_type":"code","source":"def ExtractTestInput(ImagePath):\n    img = cv.imread(ImagePath)\n    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n    img_ = cv.resize(img,(500,500))\n    hsv = cv.cvtColor(img_, cv.COLOR_BGR2HSV) #convert it to hsv\n    hsv[...,2] = hsv[...,2]*0.2\n    img1 = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n    Noise = noisy(\"s&p\",img1)\n    Noise = Noise.reshape(1,500,500,3)\n    return Noise","metadata":{"_uuid":"bb73a8bcb6d52e538248b2ce7ff6c487138afe3f","execution":{"iopub.status.busy":"2022-08-12T10:16:43.655941Z","iopub.execute_input":"2022-08-12T10:16:43.656499Z","iopub.status.idle":"2022-08-12T10:16:43.666432Z","shell.execute_reply.started":"2022-08-12T10:16:43.656443Z","shell.execute_reply":"2022-08-12T10:16:43.665364Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"ImagePath=TestPath+\"0 (6).jpg\"\nimage_for_test = ExtractTestInput(ImagePath)\nPrediction = Model_Enhancer.predict(image_for_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:43.667847Z","iopub.execute_input":"2022-08-12T10:16:43.668480Z","iopub.status.idle":"2022-08-12T10:16:43.864129Z","shell.execute_reply.started":"2022-08-12T10:16:43.668431Z","shell.execute_reply":"2022-08-12T10:16:43.863245Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"Prediction = Prediction.reshape(500,500,3)\nplt.imshow(Prediction)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:43.865683Z","iopub.execute_input":"2022-08-12T10:16:43.866155Z","iopub.status.idle":"2022-08-12T10:16:44.099689Z","shell.execute_reply.started":"2022-08-12T10:16:43.866101Z","shell.execute_reply":"2022-08-12T10:16:44.098199Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Comparating enhanced image and original output image.","metadata":{"_uuid":"fae272eb18cb9ad3eb32707c29b5ccebdb96e4bb"}},{"cell_type":"code","source":"Image_test=TestPath+\"0 (6).jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"_uuid":"8954bfb4c4bbe044230a681633b082f2940325fa","execution":{"iopub.status.busy":"2022-08-12T10:16:44.101350Z","iopub.execute_input":"2022-08-12T10:16:44.101618Z","iopub.status.idle":"2022-08-12T10:16:45.786706Z","shell.execute_reply.started":"2022-08-12T10:16:44.101570Z","shell.execute_reply":"2022-08-12T10:16:45.786019Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## **Repeating for different inputs from different test directories.**","metadata":{}},{"cell_type":"markdown","source":"### From the architecture directory.","metadata":{}},{"cell_type":"code","source":"TestPath2=\"../input/image-classification/validation/validation/architecture/\"","metadata":{"_uuid":"74700d8b54b3127bcb1b9fbccba467c2f59c9b1a","execution":{"iopub.status.busy":"2022-08-12T10:16:45.787922Z","iopub.execute_input":"2022-08-12T10:16:45.788380Z","iopub.status.idle":"2022-08-12T10:16:45.792099Z","shell.execute_reply.started":"2022-08-12T10:16:45.788333Z","shell.execute_reply":"2022-08-12T10:16:45.791164Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"Image_test2= TestPath2 + \"04_Monasterio_de_Palazuelos_absides_exterior_ni.jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test2)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test2)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"_uuid":"41ade57db60d6a3875a0716c890cd060c635d41d","execution":{"iopub.status.busy":"2022-08-12T10:16:45.793250Z","iopub.execute_input":"2022-08-12T10:16:45.793746Z","iopub.status.idle":"2022-08-12T10:16:47.453523Z","shell.execute_reply.started":"2022-08-12T10:16:45.793626Z","shell.execute_reply":"2022-08-12T10:16:47.452440Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"Image_test2= TestPath2 + \"80px-02-Abbaye_de_la_Trinite.jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test2)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test2)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:47.454788Z","iopub.execute_input":"2022-08-12T10:16:47.455362Z","iopub.status.idle":"2022-08-12T10:16:49.247744Z","shell.execute_reply.started":"2022-08-12T10:16:47.455308Z","shell.execute_reply":"2022-08-12T10:16:49.247071Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"Image_test2= TestPath2 + \"80px-Bosjo_monastery_in_Skane_Sweden5.jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test2)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test2)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:49.248968Z","iopub.execute_input":"2022-08-12T10:16:49.249464Z","iopub.status.idle":"2022-08-12T10:16:50.956770Z","shell.execute_reply.started":"2022-08-12T10:16:49.249403Z","shell.execute_reply":"2022-08-12T10:16:50.956104Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### From the food dierctory under validation.","metadata":{}},{"cell_type":"code","source":"TestPath3 = \"../input/image-classification/validation/validation/food/\"","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:50.957894Z","iopub.execute_input":"2022-08-12T10:16:50.958384Z","iopub.status.idle":"2022-08-12T10:16:50.962244Z","shell.execute_reply.started":"2022-08-12T10:16:50.958335Z","shell.execute_reply":"2022-08-12T10:16:50.961371Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"Image_test3 = TestPath3+\"0FoxSkMXWsyYKN5EqzXC.jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test3)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test3)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"_uuid":"a23b8948dcc633254e1b785bd5fd77ac14fb2574","execution":{"iopub.status.busy":"2022-08-12T10:16:50.963381Z","iopub.execute_input":"2022-08-12T10:16:50.963866Z","iopub.status.idle":"2022-08-12T10:16:52.623514Z","shell.execute_reply.started":"2022-08-12T10:16:50.963759Z","shell.execute_reply":"2022-08-12T10:16:52.621815Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"Image_test3 = TestPath3+\"0eQufX5tPtmJfGjwBS4M.jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test3)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test3)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"_uuid":"26aef16933d47a1242487460269b580e2cd218f5","execution":{"iopub.status.busy":"2022-08-12T10:16:52.624808Z","iopub.execute_input":"2022-08-12T10:16:52.625287Z","iopub.status.idle":"2022-08-12T10:16:54.270665Z","shell.execute_reply.started":"2022-08-12T10:16:52.625238Z","shell.execute_reply":"2022-08-12T10:16:54.266415Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### From the travel and adventure directory.","metadata":{}},{"cell_type":"code","source":"TestPath4 = \"../input/image-classification/validation/validation/travel and adventure/\"","metadata":{"execution":{"iopub.status.busy":"2022-08-12T10:16:54.272101Z","iopub.execute_input":"2022-08-12T10:16:54.272576Z","iopub.status.idle":"2022-08-12T10:16:54.276520Z","shell.execute_reply.started":"2022-08-12T10:16:54.272527Z","shell.execute_reply":"2022-08-12T10:16:54.275576Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"Image_test4=TestPath4+\"10.jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test4)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test4)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"_uuid":"ed2331224a49c5cc2d90dca1bb76c477cf2d5e19","execution":{"iopub.status.busy":"2022-08-12T10:16:54.277791Z","iopub.execute_input":"2022-08-12T10:16:54.278379Z","iopub.status.idle":"2022-08-12T10:16:55.894500Z","shell.execute_reply.started":"2022-08-12T10:16:54.278310Z","shell.execute_reply":"2022-08-12T10:16:55.892972Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"Image_test4=TestPath4+\"18.jpg\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test4)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test4)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"_uuid":"e0630bbbe896e34f1166df0a63a1ed46e3945d38","execution":{"iopub.status.busy":"2022-08-12T10:16:55.895751Z","iopub.execute_input":"2022-08-12T10:16:55.896378Z","iopub.status.idle":"2022-08-12T10:16:57.541750Z","shell.execute_reply.started":"2022-08-12T10:16:55.896312Z","shell.execute_reply":"2022-08-12T10:16:57.540198Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"Image_test4=TestPath4+\"16.png\"\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nimg_1 = cv.imread(Image_test4)\nimg_1 = cv.cvtColor(img_1, cv.COLOR_BGR2RGB)\nimg_1 = cv.resize(img_1, (500, 500))\nplt.title(\"Ground Truth\",fontsize=20)\nplt.imshow(img_1)\n\nplt.subplot(5,5,1+1)\nimg_ = ExtractTestInput(Image_test4)\nPrediction = Model_Enhancer.predict(img_)\nimg_ = img_.reshape(500,500,3)\nplt.title(\"Low Light Image\",fontsize=20)\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nPrediction = Prediction.reshape(500,500,3)\nimg_[:,:,:] = Prediction[:,:,:]\nplt.title(\"Enhanced Image\",fontsize=20)\nplt.imshow(img_)","metadata":{"_uuid":"c508f2277cc324da328a29ad4e9b9e9f48fb69cd","execution":{"iopub.status.busy":"2022-08-12T10:16:57.543098Z","iopub.execute_input":"2022-08-12T10:16:57.543690Z","iopub.status.idle":"2022-08-12T10:16:59.393644Z","shell.execute_reply.started":"2022-08-12T10:16:57.543605Z","shell.execute_reply":"2022-08-12T10:16:59.392405Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n1. Converted an image to low light noise filled version and later enhance it using Inception_ResNet_v2, a CNN architecture.\n2. Used \"sgd\" as optimizer and \"relu\" as the Layer activation function.\n3. The addition of salt and pepper noise results in imprecise enhancement. So we added real life low light images clicked from phones and the results were satisfactory.","metadata":{}},{"cell_type":"markdown","source":"## Reference:\n1. This report was inspired by a kaggle notebook curated by Victor Basu. You can acces it  [here](https://www.kaggle.com/code/basu369victor/low-light-image-enhancement-with-cnn/notebook)\n2. Information regarding keras was obtained for the [keras documentation website](https://keras.io/)\n","metadata":{}}]}